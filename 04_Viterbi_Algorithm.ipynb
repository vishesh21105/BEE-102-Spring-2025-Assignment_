{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a0ca61",
   "metadata": {},
   "source": [
    "VITERBI - \n",
    "\n",
    "The Viterbi algorithm is a dynamic programming method used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM), given a sequence of observed events. It efficiently computes the path with the highest probability by keeping track of the best choices at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802f1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --- HMM definition ---\n",
    "\n",
    "STATES = ['E', 'F', 'I']                  # E=Exon, F=5'UTR, I=Intron\n",
    "SYMBOLS = ['A', 'C', 'G', 'T']\n",
    "\n",
    "# start probabilities (we always begin in Exon)\n",
    "PI = {'E': 1.0, 'F': 0.0, 'I': 0.0}\n",
    "\n",
    "# transition probabilities, including a synthetic 'Begin' and 'End'\n",
    "TRANS = {\n",
    "    'Begin': {'E': 1.0, 'F': 0.0, 'I': 0.0, 'End': 0.0},\n",
    "    'E':     {'E': 0.9, 'F': 0.1, 'I': 0.0, 'End': 0.0},\n",
    "    'F':     {'E': 0.0, 'F': 0.0, 'I': 1.0, 'End': 0.0},\n",
    "    'I':     {'E': 0.0, 'F': 0.0, 'I': 0.9, 'End': 0.1},\n",
    "}\n",
    "\n",
    "# emission probabilities per state\n",
    "EMIT = {\n",
    "    'E': {'A':0.25, 'C':0.25, 'G':0.25, 'T':0.25},\n",
    "    'F': {'A':0.05, 'C':0.00, 'G':0.95, 'T':0.00},\n",
    "    'I': {'A':0.40, 'C':0.10, 'G':0.10, 'T':0.40},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce62a39",
   "metadata": {},
   "source": [
    "Our goal is to calculate the probability of seeing a particular sequence when following a given path of states in a Hidden Markov Model (HMM). Because these probabilities can become extremely small, we compute the natural logarithm of the probability instead.\n",
    "\n",
    "The total probability is found by multiplying together all the transition and emission probabilities along the chosen path. When working in log-space, this multiplication turns into addition, meaning the overall log probability is simply the sum of the log of each transition and emission probability used in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9a0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log prob = -41.22\n"
     ]
    }
   ],
   "source": [
    "def ln(x: float) -> float:\n",
    "    \"\"\"Return natural log, mapping 0 → -∞.\"\"\"\n",
    "    return -math.inf if x == 0.0 else math.log(x)\n",
    "\n",
    "def compute_log_likelihood(state_sequence: str, dna_sequence: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute log P(path, sequence) under our HMM.\n",
    "    \n",
    "    Parameters:\n",
    "      state_sequence: e.g. \"EEEEFFIIII...\"\n",
    "      dna_sequence:    same length string over 'A','C','G','T'\n",
    "    \n",
    "    Returns:\n",
    "      Sum of log-transitions + log-emissions + final log-transition to End.\n",
    "    \"\"\"\n",
    "    if len(state_sequence) != len(dna_sequence):\n",
    "        raise ValueError(\"Path and observed string must match in length.\")\n",
    "    \n",
    "    total_log_prob = 0.0\n",
    "    prev = 'Begin'\n",
    "    \n",
    "    for state, nucleotide in zip(state_sequence, dna_sequence):\n",
    "        # add log transition\n",
    "        total_log_prob += ln(TRANS[prev][state])\n",
    "        # add log emission\n",
    "        total_log_prob += ln(EMIT[state][nucleotide])\n",
    "        prev = state\n",
    "    \n",
    "    total_log_prob += ln(TRANS[prev]['End'])\n",
    "    return total_log_prob\n",
    "\n",
    "# run exmample\n",
    "if __name__ == \"__main__\":\n",
    "    path  = \"EEEEEEEEEEEEEEEEEEFIIIIIII\"\n",
    "    seq   = \"CTTCATGTGAAAGCAGACGTAAGTCA\"\n",
    "    score = compute_log_likelihood(path, seq)\n",
    "    print(f\"Log prob = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097d7651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEEEEEEEEEEEEEEEEEEEEEEEEE\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Re‐label your model parameters:\n",
    "hidden_states = ['E', '5', 'I']\n",
    "start_prob    = {'E': 1.0, '5': 0.0, 'I': 0.0}\n",
    "trans_prob    = {\n",
    "    'E': {'E': 0.9, '5': 0.1, 'I': 0.0, 'End': 0.0},\n",
    "    '5': {'E': 0.0, '5': 0.0, 'I': 1.0, 'End': 0.0},\n",
    "    'I': {'E': 0.0, '5': 0.0, 'I': 0.9, 'End': 0.1},\n",
    "}\n",
    "emit_prob     = {\n",
    "    'E': {'A':0.25, 'C':0.25, 'G':0.25, 'T':0.25},\n",
    "    '5': {'A':0.05, 'C':0.00, 'G':0.95, 'T':0.00},\n",
    "    'I': {'A':0.40, 'C':0.10, 'G':0.10, 'T':0.40},\n",
    "}\n",
    "\n",
    "# Pre‐compute logs (map zeros → –inf)\n",
    "def safe_log(x):\n",
    "    return -math.inf if x == 0.0 else math.log(x)\n",
    "\n",
    "log_start = {s: safe_log(p) for s,p in start_prob.items()}\n",
    "log_trans = {s:{t: safe_log(p2) for t,p2 in trans_prob[s].items()} for s in trans_prob}\n",
    "log_emit  = {s:{o: safe_log(e) for o,e in emit_prob[s].items()} for s in emit_prob}\n",
    "\n",
    "\n",
    "def viterbi_decode(observations):\n",
    "    \"\"\"\n",
    "    Return the most likely hidden‐state path for the given observations.\n",
    "    \n",
    "    We work in log‐space:\n",
    "      δ[t][s] = max log‐prob of any path ending in state s at time t  \n",
    "      ψ[t][s] = argmax previous state\n",
    "    \"\"\"\n",
    "    T = len(observations)\n",
    "    \n",
    "    # δ and ψ tables\n",
    "    delta = [ { s: None for s in hidden_states } for _ in range(T) ]\n",
    "    psi   = [ { s: None for s in hidden_states } for _ in range(T) ]\n",
    "    \n",
    "    # Initialization (t = 0)\n",
    "    for s in hidden_states:\n",
    "        delta[0][s] = log_start[s] + log_emit[s][observations[0]]\n",
    "        psi[0][s]   = None\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, T):\n",
    "        obs = observations[t]\n",
    "        for curr in hidden_states:\n",
    "            best_prev, best_score = None, -math.inf\n",
    "            for prev in hidden_states:\n",
    "                score = delta[t-1][prev] + log_trans[prev].get(curr, -math.inf) + log_emit[curr][obs]\n",
    "                if score > best_score:\n",
    "                    best_prev, best_score = prev, score\n",
    "            delta[t][curr] = best_score\n",
    "            psi[t][curr]   = best_prev\n",
    "    \n",
    "    # Termination: pick best final state\n",
    "    final_state, final_score = max(delta[T-1].items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Path backtracking\n",
    "    path = [final_state]\n",
    "    for t in range(T-1, 0, -1):\n",
    "        path.append( psi[t][path[-1]] )\n",
    "    path.reverse()\n",
    "    \n",
    "    return \"\".join(path)\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    seq = \"CTTCATGTGAAAGCAGACGTAAGTCA\"\n",
    "    result = viterbi_decode(seq)\n",
    "    print(result)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
